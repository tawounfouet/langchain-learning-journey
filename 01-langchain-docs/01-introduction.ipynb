{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a7be7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0094d132-e722-4024-9d8a-1628d1e9df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf201cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "print(\"Model initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef717fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_dbaca60df0', 'id': 'chatcmpl-BPQ7URwxATbNErV9PY82vUSMQFGao', 'finish_reason': 'stop', 'logprobs': None}, id='run-3e6f7cc1-072c-4297-bbbc-bc3c79e1a2ef-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc91bd-3fcf-47f0-98fb-f8366604f886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d206095a-32de-4687-8a81-bb96f1a06e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6480cae-bcd5-428d-ac8f-dcf913e304bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69835b5b-c0be-4c08-b222-ac3a45cf4d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad9b00d6-26f4-4b03-a3ab-e3a0675d11c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did ChatGPT go to therapy?', punchline='Because it had too many unresolved queries!', rating=8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = model.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about ChatGPT, haha \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecb915-c1df-44d7-a08e-64b9918f8806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d146fe95-b97c-449e-97cd-038bbef6547d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "runnable = RunnableLambda(lambda x: str(x))\n",
    "runnable.invoke(10)\n",
    "\n",
    "# Async variant:\n",
    "# await runnable.ainvoke(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c0c4e-26b2-47e4-8cf1-d9492f71aafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78884f3-fa3d-42b1-80c4-59acb89f4620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "sequence.invoke(1) # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73944bf7-23a3-4406-84e3-8c8f0535e9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 8]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.batch([1, 2, 3]) # [4, 6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a6af7c5-7638-490c-a964-c5a57e91d429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mul_2': 4, 'mul_5': 10}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
    "    'mul_5': RunnableLambda(lambda x: x * 5)\n",
    "}\n",
    "sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6babe-e62d-40a6-89fb-511a9ca01754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "998c224f-af7e-4f90-8f67-98227b5c7c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'add_one_input', 'type': 'integer'}\n",
      "{'title': 'buggy_double_output', 'type': 'integer'}\n",
      "\n",
      "This code failed, and will probably be retried!\n",
      "This code failed, and will probably be retried!\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "import random\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def buggy_double(y: int) -> int:\n",
    "    \"\"\"Buggy code that will fail 70% of the time\"\"\"\n",
    "    if random.random() > 0.3:\n",
    "        print('This code failed, and will probably be retried!')  # noqa: T201\n",
    "        raise ValueError('Triggered buggy code')\n",
    "    return y * 2\n",
    "\n",
    "sequence = (\n",
    "    RunnableLambda(add_one) |\n",
    "    RunnableLambda(buggy_double).with_retry( # Retry on failure\n",
    "        stop_after_attempt=10,\n",
    "        wait_exponential_jitter=False\n",
    "    )\n",
    ")\n",
    "\n",
    "print(sequence.input_schema.model_json_schema()) # Show inferred input schema\n",
    "print(sequence.output_schema.model_json_schema()) # Show inferred output schema\n",
    "print()\n",
    "print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4b4be-8a7d-4d31-af63-896406483f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e64ec7fd-6c88-42d1-9def-5eb73dd63c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install grandalf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0914b516-f3be-480c-b48b-82b6fc48c77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +-------------+          \n",
      "        | LambdaInput |          \n",
      "        +-------------+          \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "           +--------+            \n",
      "           | Lambda |            \n",
      "           +--------+            \n",
      "                *                \n",
      "                *                \n",
      "                *                \n",
      "+-----------------------------+  \n",
      "| Parallel<second,third>Input |  \n",
      "+-----------------------------+  \n",
      "           *         *           \n",
      "         **           **         \n",
      "        *               *        \n",
      " +--------+          +--------+  \n",
      " | Lambda |          | Lambda |  \n",
      " +--------+          +--------+  \n",
      "           *         *           \n",
      "            **     **            \n",
      "              *   *              \n",
      "+------------------------------+ \n",
      "| Parallel<second,third>Output | \n",
      "+------------------------------+ \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "runnable3 = RunnableLambda(lambda x: str(x))\n",
    "\n",
    "chain = runnable1 | RunnableParallel(second=runnable2, third=runnable3)\n",
    "\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be4bdc-1be3-401c-979e-2080f984e636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
